{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from models.darknet import Darknet\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/paperspace/code/fontastic/data/stratified/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(PATH, 'train')\n",
    "test_path = os.path.join(PATH, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/code/fontastic/data/stratified/train'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fahkwang', 'MajorMonoDisplay', 'Lato', 'Merriweather', 'Lora', 'Lobster']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_path, \n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=test_path, \n",
    "                                        transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(trainloader.dataset.classes)\n",
    "print(trainloader.dataset.class_to_idx)\n",
    "print(' '.join('%5s' % trainloader.dataset.classes[labels[j]] for j in range(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "m = Darknet([1, 2, 4, 6, 3], num_classes=len(trainloader.dataset.classes), nf=16)\n",
    "m = nn.DataParallel(m, device_ids=None)\n",
    "# m = nn.DataParallel(m, device_ids=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "net = m.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-02 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-02 * 0.1 * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(i, lr=None, maxlr=1e-03):\n",
    "    \n",
    "    if i == 0:\n",
    "        lr = maxlr * 0.1 * 0.1\n",
    "    elif i <30:\n",
    "        lr = lr / 0.1\n",
    "    else:\n",
    "        lr = lr * 0.1\n",
    "    print(f\"new lr {lr}\")\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader.dataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, mini-batch    10] loss: 2.185\n",
      "[epoch 1, mini-batch    20] loss: 1.739\n",
      "[epoch 1, mini-batch    30] loss: 1.520\n",
      "[epoch 1, mini-batch    40] loss: 1.570\n",
      "[epoch 1, mini-batch    50] loss: 1.425\n",
      "[epoch 1, mini-batch    60] loss: 1.526\n",
      "[epoch 2, mini-batch    10] loss: 1.381\n",
      "[epoch 2, mini-batch    20] loss: 1.358\n",
      "[epoch 2, mini-batch    30] loss: 1.375\n",
      "[epoch 2, mini-batch    40] loss: 1.463\n",
      "[epoch 2, mini-batch    50] loss: 1.268\n",
      "[epoch 2, mini-batch    60] loss: 1.335\n",
      "[epoch 3, mini-batch    10] loss: 1.245\n",
      "[epoch 3, mini-batch    20] loss: 1.432\n",
      "[epoch 3, mini-batch    30] loss: 1.121\n",
      "[epoch 3, mini-batch    40] loss: 1.352\n",
      "[epoch 3, mini-batch    50] loss: 1.280\n",
      "[epoch 3, mini-batch    60] loss: 1.373\n",
      "[epoch 4, mini-batch    10] loss: 1.227\n",
      "[epoch 4, mini-batch    20] loss: 1.257\n",
      "[epoch 4, mini-batch    30] loss: 1.170\n",
      "[epoch 4, mini-batch    40] loss: 1.124\n",
      "[epoch 4, mini-batch    50] loss: 1.296\n",
      "[epoch 4, mini-batch    60] loss: 1.008\n",
      "[epoch 5, mini-batch    10] loss: 1.119\n",
      "[epoch 5, mini-batch    20] loss: 1.091\n",
      "[epoch 5, mini-batch    30] loss: 1.046\n",
      "[epoch 5, mini-batch    40] loss: 1.192\n",
      "[epoch 5, mini-batch    50] loss: 1.187\n",
      "[epoch 5, mini-batch    60] loss: 1.160\n",
      "[epoch 6, mini-batch    10] loss: 1.111\n",
      "[epoch 6, mini-batch    20] loss: 1.079\n",
      "[epoch 6, mini-batch    30] loss: 1.262\n",
      "[epoch 6, mini-batch    40] loss: 1.177\n",
      "[epoch 6, mini-batch    50] loss: 1.081\n",
      "[epoch 6, mini-batch    60] loss: 1.149\n",
      "[epoch 7, mini-batch    10] loss: 1.032\n",
      "[epoch 7, mini-batch    20] loss: 1.077\n",
      "[epoch 7, mini-batch    30] loss: 1.017\n",
      "[epoch 7, mini-batch    40] loss: 1.275\n",
      "[epoch 7, mini-batch    50] loss: 1.122\n",
      "[epoch 7, mini-batch    60] loss: 1.059\n",
      "[epoch 8, mini-batch    10] loss: 1.126\n",
      "[epoch 8, mini-batch    20] loss: 0.964\n",
      "[epoch 8, mini-batch    30] loss: 1.026\n",
      "[epoch 8, mini-batch    40] loss: 1.186\n",
      "[epoch 8, mini-batch    50] loss: 1.103\n",
      "[epoch 8, mini-batch    60] loss: 1.237\n",
      "[epoch 9, mini-batch    10] loss: 1.169\n",
      "[epoch 9, mini-batch    20] loss: 1.036\n",
      "[epoch 9, mini-batch    30] loss: 0.941\n",
      "[epoch 9, mini-batch    40] loss: 1.102\n",
      "[epoch 9, mini-batch    50] loss: 0.977\n",
      "[epoch 9, mini-batch    60] loss: 1.105\n",
      "[epoch 10, mini-batch    10] loss: 1.055\n",
      "[epoch 10, mini-batch    20] loss: 1.076\n",
      "[epoch 10, mini-batch    30] loss: 1.004\n",
      "[epoch 10, mini-batch    40] loss: 0.972\n",
      "[epoch 10, mini-batch    50] loss: 1.051\n",
      "[epoch 10, mini-batch    60] loss: 1.127\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    lr = None\n",
    "#     lr = get_lr(i=0, lr=lr, maxlr=1e-04)\n",
    "#     optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        \n",
    "#         lr = get_lr(i=i, lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 200 mini-batches\n",
    "            print('[epoch %d, mini-batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            \n",
    "            #change lr\n",
    "#             lr = get_lr(i=i, lr=lr, maxlr=1e-04)\n",
    "#             optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new lr 1.0000000000000002e-07\n",
      "[epoch 1, mini-batch    10] loss: 0.943\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 1, mini-batch    20] loss: 1.058\n",
      "new lr 1e-05\n",
      "[epoch 1, mini-batch    30] loss: 0.995\n",
      "new lr 0.0001\n",
      "[epoch 1, mini-batch    40] loss: 1.028\n",
      "new lr 1e-05\n",
      "[epoch 1, mini-batch    50] loss: 0.983\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 1, mini-batch    60] loss: 0.953\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 2, mini-batch    10] loss: 1.024\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 2, mini-batch    20] loss: 1.078\n",
      "new lr 1e-05\n",
      "[epoch 2, mini-batch    30] loss: 0.982\n",
      "new lr 0.0001\n",
      "[epoch 2, mini-batch    40] loss: 1.011\n",
      "new lr 1e-05\n",
      "[epoch 2, mini-batch    50] loss: 0.913\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 2, mini-batch    60] loss: 0.930\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 3, mini-batch    10] loss: 1.041\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 3, mini-batch    20] loss: 0.929\n",
      "new lr 1e-05\n",
      "[epoch 3, mini-batch    30] loss: 1.059\n",
      "new lr 0.0001\n",
      "[epoch 3, mini-batch    40] loss: 0.962\n",
      "new lr 1e-05\n",
      "[epoch 3, mini-batch    50] loss: 0.966\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 3, mini-batch    60] loss: 0.919\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 4, mini-batch    10] loss: 0.982\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 4, mini-batch    20] loss: 0.968\n",
      "new lr 1e-05\n",
      "[epoch 4, mini-batch    30] loss: 0.880\n",
      "new lr 0.0001\n",
      "[epoch 4, mini-batch    40] loss: 0.977\n",
      "new lr 1e-05\n",
      "[epoch 4, mini-batch    50] loss: 0.971\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 4, mini-batch    60] loss: 0.849\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 5, mini-batch    10] loss: 0.833\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 5, mini-batch    20] loss: 0.909\n",
      "new lr 1e-05\n",
      "[epoch 5, mini-batch    30] loss: 0.938\n",
      "new lr 0.0001\n",
      "[epoch 5, mini-batch    40] loss: 0.953\n",
      "new lr 1e-05\n",
      "[epoch 5, mini-batch    50] loss: 0.857\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 5, mini-batch    60] loss: 0.900\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 6, mini-batch    10] loss: 0.975\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 6, mini-batch    20] loss: 0.956\n",
      "new lr 1e-05\n",
      "[epoch 6, mini-batch    30] loss: 0.951\n",
      "new lr 0.0001\n",
      "[epoch 6, mini-batch    40] loss: 1.019\n",
      "new lr 1e-05\n",
      "[epoch 6, mini-batch    50] loss: 0.835\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 6, mini-batch    60] loss: 0.990\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 7, mini-batch    10] loss: 1.013\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 7, mini-batch    20] loss: 0.894\n",
      "new lr 1e-05\n",
      "[epoch 7, mini-batch    30] loss: 1.069\n",
      "new lr 0.0001\n",
      "[epoch 7, mini-batch    40] loss: 0.925\n",
      "new lr 1e-05\n",
      "[epoch 7, mini-batch    50] loss: 0.990\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 7, mini-batch    60] loss: 1.005\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 8, mini-batch    10] loss: 0.928\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 8, mini-batch    20] loss: 0.881\n",
      "new lr 1e-05\n",
      "[epoch 8, mini-batch    30] loss: 0.962\n",
      "new lr 0.0001\n",
      "[epoch 8, mini-batch    40] loss: 0.949\n",
      "new lr 1e-05\n",
      "[epoch 8, mini-batch    50] loss: 0.978\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 8, mini-batch    60] loss: 0.917\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 9, mini-batch    10] loss: 1.006\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 9, mini-batch    20] loss: 0.866\n",
      "new lr 1e-05\n",
      "[epoch 9, mini-batch    30] loss: 0.890\n",
      "new lr 0.0001\n",
      "[epoch 9, mini-batch    40] loss: 0.861\n",
      "new lr 1e-05\n",
      "[epoch 9, mini-batch    50] loss: 0.982\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 9, mini-batch    60] loss: 0.887\n",
      "new lr 1.0000000000000002e-07\n",
      "new lr 1.0000000000000002e-07\n",
      "[epoch 10, mini-batch    10] loss: 0.919\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 10, mini-batch    20] loss: 0.929\n",
      "new lr 1e-05\n",
      "[epoch 10, mini-batch    30] loss: 0.924\n",
      "new lr 0.0001\n",
      "[epoch 10, mini-batch    40] loss: 0.937\n",
      "new lr 1e-05\n",
      "[epoch 10, mini-batch    50] loss: 0.928\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 10, mini-batch    60] loss: 0.811\n",
      "new lr 1.0000000000000002e-07\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    lr = None\n",
    "    lr = get_lr(i=0, lr=lr, maxlr=1e-04)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        \n",
    "#         lr = get_lr(i=i, lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 200 mini-batches\n",
    "            print('[epoch %d, mini-batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            \n",
    "            #change lr\n",
    "            lr = get_lr(i=i, lr=lr, maxlr=1e-04)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new lr 1.0000000000000002e-06\n",
      "[epoch 1, mini-batch    10] loss: 1.046\n",
      "new lr 1e-05\n",
      "[epoch 1, mini-batch    20] loss: 0.938\n",
      "new lr 0.0001\n",
      "[epoch 1, mini-batch    30] loss: 0.866\n",
      "new lr 0.001\n",
      "[epoch 1, mini-batch    40] loss: 0.933\n",
      "new lr 0.0001\n",
      "[epoch 1, mini-batch    50] loss: 0.881\n",
      "new lr 1e-05\n",
      "[epoch 1, mini-batch    60] loss: 0.835\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 2, mini-batch    10] loss: 0.828\n",
      "new lr 1e-05\n",
      "[epoch 2, mini-batch    20] loss: 0.831\n",
      "new lr 0.0001\n",
      "[epoch 2, mini-batch    30] loss: 0.853\n",
      "new lr 0.001\n",
      "[epoch 2, mini-batch    40] loss: 0.832\n",
      "new lr 0.0001\n",
      "[epoch 2, mini-batch    50] loss: 0.942\n",
      "new lr 1e-05\n",
      "[epoch 2, mini-batch    60] loss: 0.898\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 3, mini-batch    10] loss: 0.815\n",
      "new lr 1e-05\n",
      "[epoch 3, mini-batch    20] loss: 0.847\n",
      "new lr 0.0001\n",
      "[epoch 3, mini-batch    30] loss: 0.895\n",
      "new lr 0.001\n",
      "[epoch 3, mini-batch    40] loss: 0.947\n",
      "new lr 0.0001\n",
      "[epoch 3, mini-batch    50] loss: 0.809\n",
      "new lr 1e-05\n",
      "[epoch 3, mini-batch    60] loss: 0.837\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 4, mini-batch    10] loss: 0.918\n",
      "new lr 1e-05\n",
      "[epoch 4, mini-batch    20] loss: 0.883\n",
      "new lr 0.0001\n",
      "[epoch 4, mini-batch    30] loss: 0.769\n",
      "new lr 0.001\n",
      "[epoch 4, mini-batch    40] loss: 0.835\n",
      "new lr 0.0001\n",
      "[epoch 4, mini-batch    50] loss: 0.829\n",
      "new lr 1e-05\n",
      "[epoch 4, mini-batch    60] loss: 0.801\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 5, mini-batch    10] loss: 0.821\n",
      "new lr 1e-05\n",
      "[epoch 5, mini-batch    20] loss: 0.815\n",
      "new lr 0.0001\n",
      "[epoch 5, mini-batch    30] loss: 0.897\n",
      "new lr 0.001\n",
      "[epoch 5, mini-batch    40] loss: 0.787\n",
      "new lr 0.0001\n",
      "[epoch 5, mini-batch    50] loss: 0.765\n",
      "new lr 1e-05\n",
      "[epoch 5, mini-batch    60] loss: 0.815\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 6, mini-batch    10] loss: 0.808\n",
      "new lr 1e-05\n",
      "[epoch 6, mini-batch    20] loss: 0.778\n",
      "new lr 0.0001\n",
      "[epoch 6, mini-batch    30] loss: 0.799\n",
      "new lr 0.001\n",
      "[epoch 6, mini-batch    40] loss: 0.955\n",
      "new lr 0.0001\n",
      "[epoch 6, mini-batch    50] loss: 0.798\n",
      "new lr 1e-05\n",
      "[epoch 6, mini-batch    60] loss: 0.745\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 7, mini-batch    10] loss: 0.786\n",
      "new lr 1e-05\n",
      "[epoch 7, mini-batch    20] loss: 0.755\n",
      "new lr 0.0001\n",
      "[epoch 7, mini-batch    30] loss: 0.762\n",
      "new lr 0.001\n",
      "[epoch 7, mini-batch    40] loss: 0.772\n",
      "new lr 0.0001\n",
      "[epoch 7, mini-batch    50] loss: 0.806\n",
      "new lr 1e-05\n",
      "[epoch 7, mini-batch    60] loss: 0.825\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 8, mini-batch    10] loss: 0.749\n",
      "new lr 1e-05\n",
      "[epoch 8, mini-batch    20] loss: 0.753\n",
      "new lr 0.0001\n",
      "[epoch 8, mini-batch    30] loss: 0.867\n",
      "new lr 0.001\n",
      "[epoch 8, mini-batch    40] loss: 0.885\n",
      "new lr 0.0001\n",
      "[epoch 8, mini-batch    50] loss: 0.708\n",
      "new lr 1e-05\n",
      "[epoch 8, mini-batch    60] loss: 0.661\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 9, mini-batch    10] loss: 0.863\n",
      "new lr 1e-05\n",
      "[epoch 9, mini-batch    20] loss: 0.966\n",
      "new lr 0.0001\n",
      "[epoch 9, mini-batch    30] loss: 0.749\n",
      "new lr 0.001\n",
      "[epoch 9, mini-batch    40] loss: 0.889\n",
      "new lr 0.0001\n",
      "[epoch 9, mini-batch    50] loss: 0.680\n",
      "new lr 1e-05\n",
      "[epoch 9, mini-batch    60] loss: 0.711\n",
      "new lr 1.0000000000000002e-06\n",
      "new lr 1.0000000000000002e-06\n",
      "[epoch 10, mini-batch    10] loss: 0.651\n",
      "new lr 1e-05\n",
      "[epoch 10, mini-batch    20] loss: 0.746\n",
      "new lr 0.0001\n",
      "[epoch 10, mini-batch    30] loss: 0.711\n",
      "new lr 0.001\n",
      "[epoch 10, mini-batch    40] loss: 0.798\n",
      "new lr 0.0001\n",
      "[epoch 10, mini-batch    50] loss: 0.847\n",
      "new lr 1e-05\n",
      "[epoch 10, mini-batch    60] loss: 0.787\n",
      "new lr 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    lr = None\n",
    "    lr = get_lr(i=0, lr=lr, maxlr=1e-03)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        \n",
    "#         lr = get_lr(i=i, lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 200 mini-batches\n",
    "            print('[epoch %d, mini-batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            \n",
    "            #change lr\n",
    "            lr = get_lr(i=i, lr=lr, maxlr=1e-03)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new lr 0.0001\n",
      "[epoch 1, mini-batch    20] loss: 0.898\n",
      "new lr 0.001\n",
      "[epoch 1, mini-batch    40] loss: 0.957\n",
      "new lr 0.0001\n",
      "[epoch 1, mini-batch    60] loss: 0.967\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 2, mini-batch    20] loss: 0.917\n",
      "new lr 0.001\n",
      "[epoch 2, mini-batch    40] loss: 0.904\n",
      "new lr 0.0001\n",
      "[epoch 2, mini-batch    60] loss: 0.846\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 3, mini-batch    20] loss: 0.853\n",
      "new lr 0.001\n",
      "[epoch 3, mini-batch    40] loss: 0.998\n",
      "new lr 0.0001\n",
      "[epoch 3, mini-batch    60] loss: 0.881\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 4, mini-batch    20] loss: 0.839\n",
      "new lr 0.001\n",
      "[epoch 4, mini-batch    40] loss: 0.952\n",
      "new lr 0.0001\n",
      "[epoch 4, mini-batch    60] loss: 0.916\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 5, mini-batch    20] loss: 0.812\n",
      "new lr 0.001\n",
      "[epoch 5, mini-batch    40] loss: 0.899\n",
      "new lr 0.0001\n",
      "[epoch 5, mini-batch    60] loss: 0.960\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 6, mini-batch    20] loss: 0.833\n",
      "new lr 0.001\n",
      "[epoch 6, mini-batch    40] loss: 0.891\n",
      "new lr 0.0001\n",
      "[epoch 6, mini-batch    60] loss: 0.886\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 7, mini-batch    20] loss: 0.835\n",
      "new lr 0.001\n",
      "[epoch 7, mini-batch    40] loss: 0.867\n",
      "new lr 0.0001\n",
      "[epoch 7, mini-batch    60] loss: 0.848\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 8, mini-batch    20] loss: 0.835\n",
      "new lr 0.001\n",
      "[epoch 8, mini-batch    40] loss: 0.819\n",
      "new lr 0.0001\n",
      "[epoch 8, mini-batch    60] loss: 0.893\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 9, mini-batch    20] loss: 0.859\n",
      "new lr 0.001\n",
      "[epoch 9, mini-batch    40] loss: 0.825\n",
      "new lr 0.0001\n",
      "[epoch 9, mini-batch    60] loss: 0.858\n",
      "new lr 1e-05\n",
      "new lr 0.0001\n",
      "[epoch 10, mini-batch    20] loss: 0.814\n",
      "new lr 0.001\n",
      "[epoch 10, mini-batch    40] loss: 0.846\n",
      "new lr 0.0001\n",
      "[epoch 10, mini-batch    60] loss: 0.852\n",
      "new lr 1e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    lr = None\n",
    "    lr = get_lr(i=0, lr=lr, maxlr=1e-02)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        \n",
    "#         lr = get_lr(i=i, lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 200 mini-batches\n",
    "            print('[epoch %d, mini-batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            \n",
    "            #change lr\n",
    "            lr = get_lr(i=i, lr=lr, maxlr=1e-02)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# # print images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 0, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:   Lora Fahkwang  Lora  Lora Fahkwang  Lora  Lora  Lato Fahkwang Fahkwang Lobster  Lato Fahkwang  Lato  Lora  Lato\n",
      "Predicted:  Merriweather Fahkwang Fahkwang  Lato Fahkwang Fahkwang  Lato  Lato Fahkwang Fahkwang Lobster  Lato Fahkwang  Lato  Lato  Lato\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % trainloader.dataset.classes[labels[j]] for j in range(16)))\n",
    "print('Predicted: ', ' '.join('%5s' % trainloader.dataset.classes[predicted[j]] for j in range(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 250 test images: 66 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "all_lbls = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_lbls.extend(labels.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 250 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(all_lbls, all_preds):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337,  42,   1,   4,   0,   0],\n",
       "       [ 44, 231,   8,   5,  13,  19],\n",
       "       [  2,  15,  14,   0,   0,   1],\n",
       "       [ 48,  70,   5,   2,   0,   3],\n",
       "       [  1,  13,   0,   0,  18,   0],\n",
       "       [  1,  43,   0,   1,   0,  58]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(all_lbls, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
